# Part 1
## Q1 Q1: TensorFlow vs. PyTorch — Key Differences & When to Use
TensorFlow uses a static computation graph, meaning you define the entire model structure before running it.
PyTorch uses dynamic computation graphs, allowing you to build and modify the model on-the-fly during execution. This makes debugging and experimentation easier.

TensorFlow: Ideal for production environments, mobile/web deployment, or when using tools like TFX or TensorBoard.
PyTorch: Preferred for research, rapid prototyping, and when you want more flexibility and cleaner debugging.

## Q2: Two Use Cases for Jupyter Notebooks in AI
1. 	Exploratory Data Analysis (EDA)
Jupyter makes it easy to visualize datasets using , , or  plots inline. You can iteratively clean, transform, and inspect data with immediate feedback.
2. 	Model Prototyping and Experimentation
You can test different machine learning models, tweak hyperparameters, and document results all in one place. It’s especially useful for sharing results with collaborators or stakeholders.

## Q3: How spaCy Enhances NLP vs. Basic Python String Ops
spaCy is a powerful NLP library that goes far beyond what basic string methods like , , or  can do. Here’s how:
• 	Linguistic Awareness: spaCy understands grammar and syntax. It can identify parts of speech, named entities (like people, places), and sentence boundaries.
• 	Tokenization & Lemmatization: Instead of just splitting on spaces, spaCy intelligently breaks text into tokens and reduces words to their base forms (e.g., “running” → “run”).
• 	Efficiency: It’s optimized in Cython, making it fast and scalable for large corpora.

2. Comparative Analysis

Scikit-learn is best suited for classical machine learning tasks such as regression, classification, clustering, and dimensionality reduction using models like decision trees, support vector machines, and linear regression.
TensorFlow is designed for building and training deep learning models, especially for complex tasks involving images, text, speech, and time-series data using neural networks like CNNs, RNNs, and transformers.

Ease of Use for Beginners
- Scikit-learn:
✅ Very beginner-friendly. Simple API, minimal setup, and great for learning ML fundamentals.
- TensorFlow:
⚠️ More complex. Requires understanding of tensors, graphs, and model architecture. TensorFlow 2.x improved usability with Keras integration, but still steeper than Scikit-learn.

Community Support
- Scikit-learn:
- Strong academic and industry adoption.
- Extensive documentation and tutorials.
- Great for traditional ML workflows.
- TensorFlow:
- Backed by Google.
- Massive global community, especially in deep learning.
- Rich ecosystem (TensorBoard, TF Lite, TF Serving).

Part 3
MNIST - Overrepresentation of certain digits<br>- Uniform handwriting style (mostly from US Census workers)<br>- Poor generalization to diverse handwriting (e.g., different cultures, left-handed writing)
Amazon Reviews - Skewed sentiment due to fake or incentivized reviews<br>- Imbalanced representation across product categories or brands<br>- Language bias (e.g., non-native English phrasing misinterpreted as negative)

TensorFlow Fairness Indicators
Helps detect and visualize disparities in model performance across subgroups (e.g., gender, region, product type). For example:
from tensorflow_model_analysis.addons.fairness.view import widget_view
widget_view.render_fairness_indicator(eval_result)

spaCy Rule-Based Systems
You can define custom patterns to reduce bias in entity recognition or sentiment. For example, if spaCy mislabels “Apple” as a fruit instead of a brand, you can override it:
from spacy.tokens import Span
def custom_ner(doc):
    for ent in doc.ents:
        if ent.text == "Apple":
            ent.label_ = "ORG"
    return doc
nlp.add_pipe(custom_ner, after="ner")

2. Troubleshooting Challenge: Fixing Buggy TensorFlow Code
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, input_shape=(28, 28), activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5)

Fixed Version
# Flatten input and use sparse_categorical_crossentropy for integer labels
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5)
